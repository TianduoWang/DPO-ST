exp_name: llama-2
trainer:
  run_name: 'dpo-1'
  output_dir: 'ft_models/llama-2/dpo-1'
  overwrite_output_dir: true
  bf16: true
  tf32: true
  report_to: 'wandb'
  seed: 42
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  per_device_eval_batch_size: 4
  max_steps: 100
  optim: 'adamw_torch'
  weight_decay: 0.01
  lr_scheduler_type: 'cosine_with_min_lr'
  learning_rate: 3e-7
  lr_scheduler_kwargs:
    min_lr_rate: 0.1
  warmup_ratio: 0.1
  save_strategy: 'no'
  ddp_find_unused_parameters: false
  dataloader_num_workers: 12
  beta: 0.1
  max_prompt_length: 256
  max_target_length: 384
  max_length: 640
  eval_strategy: 'steps'
  eval_steps: 10
  logging_steps: 5
  remove_unused_columns: False

model:
  sft_run_name: sft-0
  model_path: ft_models/llama-2/sft-0

data:
  data_dir: model_outputs/${exp_name}/${model.sft_run_name}/train/
  data_path: 'gsm8k/train.jsonl'
  train_data_path: 'gsm8k/train.jsonl'
  dev_data_path: 'gsm8k/dev.jsonl'
  test_data_path: 'gsm8k/test.jsonl'
  tokenizer_path: ${model.model_path}
  model_max_length: 640
  max_src_len: 256
  max_tgt_len: 384

eval:
  per_device_eval_batch_size: 16
  max_new_tokens: 300
  use_calc: true
  mode: 'greedy'
  sampling:
    temperature: 0.7
    min_seed: 0
    max_seed: 10